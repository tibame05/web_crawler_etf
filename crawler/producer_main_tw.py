# crawler/producer_main_tw.py
import os
import pandas as pd
import shutil
from crawler.tasks_etf_list_tw import scrape_etf_list         # åŒ¯å…¥çˆ¬ ETF æ¸…å–®çš„å‡½å¼
from crawler.tasks_crawler_etf_tw import crawler_etf_daily_price, crawler_etf_dividend       # åŒ¯å…¥çˆ¬å– ETF æ­·å²åƒ¹æ ¼èˆ‡é…æ¯çš„å‡½å¼
from crawler.tasks_backtest_utils_tw import calculate_indicators, evaluate_performance      # åŒ¯å…¥æŠ€è¡“æŒ‡æ¨™èˆ‡ç¸¾æ•ˆåˆ†æçš„å‡½å¼

from database.main import (
    write_etfs_to_db,   # å¯«å…¥ ETF æ¸…å–®åˆ°è³‡æ–™åº«
    write_etf_daily_price_to_db,    # å¯«å…¥ ETF æ­·å²åƒ¹æ ¼èˆ‡æŠ€è¡“æŒ‡æ¨™åˆ°è³‡æ–™åº«
    write_etf_dividend_to_db,   # å¯«å…¥ ETF é…æ¯åˆ°è³‡æ–™åº«
    write_etf_backtest_results_to_db,   # å¯«å…¥ ETF ç¸¾æ•ˆåˆ†æåˆ°è³‡æ–™åº«
)


if __name__ == "__main__":
    # æ§åˆ¶æ˜¯å¦è¦å­˜ CSV
    SAVE_CSV = False


    # 0ï¸âƒ£ å…ˆçˆ¬ ETF æ¸…å–®ï¼ˆåç¨±èˆ‡ä»£è™Ÿï¼‰ï¼Œä¸¦å„²å­˜æˆ etf_list.csv
    print("é–‹å§‹ 0ï¸âƒ£ çˆ¬ ETF æ¸…å–®")
    etfs_df = scrape_etf_list.apply_async(kwargs={"save_csv": SAVE_CSV}).get()
    print(f"âœ… çˆ¬å–åˆ°æ‰€æœ‰ ETF list")
    write_etfs_to_db(etfs_df)
    print("âœ… ETF æ¸…å–®å·²å„²å­˜åˆ°è³‡æ–™åº«")


    # 1ï¸âƒ£~3ï¸âƒ£ æ¯ä¸€æ”¯ ETF éƒ½è¦åšçš„äº‹æƒ…
    print("é–‹å§‹ ğŸ§© è™•ç†æ‰€æœ‰ ETF è³‡æ–™")
    ticker_list = etfs_df["etf_id"].dropna().tolist()

    for ticker in ticker_list:
        print(f"\nğŸ¯ è™•ç†ï¼š{ticker}")

        try:
            # === 1ï¸âƒ£ æŠ“æ­·å²åƒ¹æ ¼ + æŠ€è¡“æŒ‡æ¨™è¨ˆç®— ===
            df_price = crawler_etf_daily_price.apply_async(args=[ticker]).get()
            print(df_price.head())   # â† æª¢æŸ¥æ­·å²åƒ¹æ ¼è³‡æ–™
            #print(df_price.columns)  # â† ç¢ºèªæ¬„ä½åç¨±å°ä¸å°
            if df_price.empty:
                print(f"âš ï¸ ç„¡æ­·å²åƒ¹æ ¼è³‡æ–™ï¼š{ticker}")
                continue

            # æª¢æŸ¥æ—¥æœŸæ¬„ä½æ˜¯å¦ç‚º datetime æ ¼å¼ï¼Œå†åŠ ä¸ŠæŠ€è¡“æŒ‡æ¨™æ¬„ä½
            df_price["date"] = pd.to_datetime(df_price["date"])
            df_combined = calculate_indicators.apply_async(args=[df_price]).get()
            df_combined["date"] = pd.to_datetime(df_combined["date"])
            print(df_combined[["adj_close", "daily_return", "cumulative_return"]].tail())

            # åˆªé™¤åƒ¹æ ¼æ¬„ç‚º NaN çš„è³‡æ–™ï¼ˆé¿å…å¯«å…¥å…¨ç©º rowï¼‰
            key_cols = ["adj_close", "close", "high", "low", "open", "volume", "daily_return", "cumulative_return"]
            df_combined.dropna(subset=key_cols, how="all", inplace=True)

            # è‹¥æ¸…å®Œç‚ºç©ºï¼Œå°±ä¸è¦å¯«å…¥
            if df_combined.empty:
                print(f"âš ï¸ æ¸…é™¤ NaN å¾Œç„¡æœ‰æ•ˆåƒ¹æ ¼è³‡æ–™ï¼š{ticker}")
                continue
            write_etf_daily_price_to_db(df_combined)
            print(f"âœ… {ticker} æ­·å²åƒ¹æ ¼èˆ‡æŠ€è¡“æŒ‡æ¨™å·²å„²å­˜")

            # === 2ï¸âƒ£ æŠ“é…æ¯è³‡æ–™ ===
            df_dividend = crawler_etf_dividend.apply_async(args=[ticker]).get()
            if not df_dividend.empty:
                write_etf_dividend_to_db(df_dividend)
                print(f"âœ… {ticker} é…æ¯è³‡æ–™å·²å„²å­˜")
            else:
                print(f"âš ï¸ ç„¡é…æ¯è³‡æ–™ï¼š{ticker}")

            # === 3ï¸âƒ£ ç¸¾æ•ˆåˆ†æ ===
            metrics = evaluate_performance.apply_async(args=[df_combined]).get() 
            if metrics is not None:
                metrics["etf_id"] = ticker

                desired_order = [
                    "etf_id", "backtest_start", "backtest_end",
                    "total_return", "cagr", "max_drawdown", "sharpe_ratio"
                ]
                df_metrics = pd.DataFrame([metrics])  
                df_metrics = df_metrics[[col for col in desired_order if col in df_metrics]]
                write_etf_backtest_results_to_db(df_metrics)
                print(f"âœ… {ticker} ç¸¾æ•ˆåˆ†æå·²å„²å­˜")
            else:
                print(f"âš ï¸ ç„¡æ³•é€²è¡Œç¸¾æ•ˆåˆ†æï¼š{ticker}")

        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{ticker} - {e}")

    print("âœ… å…¨éƒ¨ ETF è³‡æ–™è™•ç†å®Œæˆ")

